# -*- coding: utf-8 -*-
"""code_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DpMHOxsrer5jiYi1eItYzp6HLjTHjzb_
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras.layers import Input, Embedding, Lambda, Dense
from tensorflow.keras.models import Model

# Sample datasets
names = ['Sue', 'Bob', 'Alice']
skills1 = ['Python', 'SQL', 'JavaScript', 'Linux', 'C++']
X1_skills = [[0], [1], [2]]

skills2 = ['Python', 'Linux', 'C++', 'Go','C']
X2_skills = [[0], [1], [3]]

# Convert inputs to arrays
X1_skills = np.array(X1_skills)
X2_skills = np.array(X2_skills)

# Generate target skill pairs
skill_pairs = np.array([
    [0, 0], # Python, Python - similar
    [1, 3]  # SQL, Go - different
])

# Create target labels
num_pairs = len(skill_pairs)
same_skill_labels = np.zeros(num_pairs)

for i in range(num_pairs):
    if skill_pairs[i, 0] == skill_pairs[i, 1]:
        same_skill_labels[i] = 1

# Siamese network model
input1 = Input((1,))
input2 = Input((1,))

emb1 = Embedding(len(set(skills1)), 50)(input1)
emb2 = Embedding(len(set(skills2)), 50)(input2)

L1_dist = Lambda(abs)(emb1 - emb2)
pred = Dense(1, 'sigmoid')(L1_dist)

model = Model(inputs=[input1, input2], outputs=pred)

# Compile and train model
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit([X1_skills, X2_skills], same_skill_labels, epochs=100)

# Get skill embeddings
emb1 = model.predict(X1_skills)
emb2 = model.predict(X2_skills)

# Find closest skill matches
scores = cosine_similarity(emb2, emb1)

top_matches = []
for i, emb2 in enumerate(emb2):
    top_idx = np.argmax(scores[i])
    name = names[top_idx]
    score = scores[i, top_idx]

    top_matches.append((name, score))

top_matches.sort(key=lambda x: x[1], reverse=True)

print(top_matches)

len(X1_skills)

len(X2_skills)

len(same_skill_labels)

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Sample datasets
names = ['Sue', 'Bob', 'Alice','Jake','Sam']
skills1 = ['Python', 'SQL', 'JavaScript', 'Linux', 'C++']
skills2 = ['Python', 'Linux', 'C++', 'Go', 'C']

# Create a dictionary to map skill2 to the corresponding name
name_dict = {skill2: name for skill2, name in zip(skills2, names)}

# Compute skill embeddings
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
skill_embeddings = vectorizer.fit_transform(skills1 + skills2)

# Calculate cosine similarity
similarity_matrix = cosine_similarity(skill_embeddings)

# Find closest skill matches
top_matches = []

for i, skill2 in enumerate(skills2):
    closest_idx = np.argmax(similarity_matrix[i][:len(skills1)])  # Only consider skills from skills1
    closest_skill = skills1[closest_idx]
    similarity_score = similarity_matrix[i][closest_idx]

    # Get the name associated with skill2, or use a default value if not found
    person_name = name_dict.get(skill2, 'Unknown')

    top_matches.append((person_name, skill2, closest_skill, similarity_score))

top_matches.sort(key=lambda x: x[3], reverse=True)

for match in top_matches:
    print(f"Person: {match[0]}, Skill 2: {match[1]}, Closest Skill 1: {match[2]}, Similarity Score: {match[3]}")

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample datasets
names = ['Sue', 'Bob', 'Alice']
skills1 = ['Python', 'SQL', 'JavaScript', 'Linux', 'C++']
skills2 = ['Python', 'Linux', 'C++', 'Go', 'C']

# Create a DataFrame
data = {
    'Name': names,
    'Skills1': skills1,
    'Skills2': skills2
}

df = pd.DataFrame(data)

# Create a dictionary to map skill2 to the corresponding name
name_dict = {skill2: name for name, skill2 in zip(df['Name'], df['Skills2'])}

# Compute skill embeddings
vectorizer = TfidfVectorizer()
skill_embeddings = vectorizer.fit_transform(df['Skills1'] + df['Skills2'])

# Calculate cosine similarity
similarity_matrix = cosine_similarity(skill_embeddings)

# Find closest skill matches
top_matches = []

for i, skill2 in enumerate(df['Skills2']):
    closest_idx = np.argmax(similarity_matrix[i][:len(df['Skills1'])])  # Only consider skills from Skills1
    closest_skill = df['Skills1'].iloc[closest_idx]
    similarity_score = similarity_matrix[i][closest_idx]

    # Get the name associated with skill2
    person_name = name_dict.get(skill2, 'Unknown')

    top_matches.append((person_name, skill2, closest_skill, similarity_score))

top_matches.sort(key=lambda x: x[3], reverse=True)

# Create a Pandas DataFrame for the top matches
top_matches_df = pd.DataFrame(top_matches, columns=['Person', 'Skill2', 'Closest Skill1', 'Similarity Score'])

# Display the DataFrame with top matches
print(top_matches_df)

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample datasets
names = ['Sue', 'Bob', 'Alice']
skills1 = ['Python', 'SQL', 'JavaScript', 'Linux', 'C++']
skills2 = ['Python', 'Linux', 'C++', 'Go', 'C']

# Ensure that names, skills1, and skills2 are of equal length by taking the minimum length
min_length = min(len(names), len(skills1), len(skills2))

# Create DataFrames with equal-length data
data1 = {
    'Name': names[:min_length],
    'Skills1': skills1[:min_length]
}

data2 = {
    'Skills2': skills2[:min_length]
}

df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)

# Create a dictionary to map skill2 to the corresponding name
name_dict = {skill2: name for name, skill2 in zip(df1['Name'], df2['Skills2'])}

# Compute skill embeddings
vectorizer = TfidfVectorizer()
skill_embeddings = vectorizer.fit_transform(df1['Skills1'] + df2['Skills2'])

# Calculate cosine similarity
similarity_matrix = cosine_similarity(skill_embeddings)

# Find closest skill matches
top_matches = []

for i, skill2 in enumerate(df2['Skills2']):
    closest_idx = np.argmax(similarity_matrix[i][:min_length])  # Only consider skills from Skills1
    closest_skill = df1['Skills1'].iloc[closest_idx]
    similarity_score = similarity_matrix[i][closest_idx]

    # Get the name associated with skill2
    person_name = name_dict.get(skill2, 'Unknown')

    top_matches.append((person_name, skill2, closest_skill, similarity_score))

top_matches.sort(key=lambda x: x[3], reverse=True)

# Create a Pandas DataFrame for the top matches
top_matches_df = pd.DataFrame(top_matches, columns=['Person', 'Skill2', 'Closest Skill1', 'Similarity Score'])

# Display the DataFrame with top matches
print(top_matches_df)

df1

df2

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample datasets
names = ['Sue', 'Bob', 'Alice']
skills1 = ['Python', 'Python', 'JavaScript', 'Linux', 'C++']
skills2 = ['Python', 'R', 'C++', 'Go', 'C']

# Generate additional names and corresponding skills for skills1
additional_names = ['John', 'Mary', 'David', 'Sarah', 'Michael']
additional_skills1 = ['Java', 'HTML', 'CSS', 'Ruby', 'PHP']

# Extend names and skills1
names.extend(additional_names)
skills1.extend(additional_skills1)

# Ensure that names, skills1, and skills2 are of equal length by taking the minimum length
min_length = min(len(names), len(skills1), len(skills2))

# Create DataFrames with equal-length data
data1 = {
    'Name': names[:min_length],
    'Skills1': skills1[:min_length]
}

data2 = {
    'Skills2': skills2[:min_length]
}

df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)

# Create a dictionary to map skill2 to the corresponding name
name_dict = {skill2: name for name, skill2 in zip(df1['Name'], df2['Skills2'])}

# Compute skill embeddings
vectorizer = TfidfVectorizer()
skill_embeddings = vectorizer.fit_transform(df1['Skills1'] + df2['Skills2'])

# Calculate cosine similarity
similarity_matrix = cosine_similarity(skill_embeddings)

# Find closest skill matches
top_matches = []

for i, skill2 in enumerate(df2['Skills2']):
    closest_idx = np.argmax(similarity_matrix[i][:min_length])  # Only consider skills from Skills1
    closest_skill = df1['Skills1'].iloc[closest_idx]
    similarity_score = similarity_matrix[i][closest_idx]

    # Get the name associated with skill2
    person_name = name_dict.get(skill2, 'Unknown')

    top_matches.append((person_name, skill2, closest_skill, similarity_score))

top_matches.sort(key=lambda x: x[3], reverse=True)

# Create a Pandas DataFrame for the top matches
top_matches_df = pd.DataFrame(top_matches, columns=['Person', 'Skill2', 'Closest Skill1', 'Similarity Score'])

# Display the DataFrame with top matches
print(top_matches_df)

df1

df2

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Lambda, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from sklearn.preprocessing import LabelEncoder

# # Sample datasets
# names = ['Gautam Galada','Pranav']
# skills1 = ['AI', 'CLOUD','BACKEND', 'FRONTEND','FULLSTACK','WEB DEV','APP DEV','CYBER SECURITY']
# skills2 = ['AI', 'CLOUD','BACKEND', 'FRONTEND','FULLSTACK','WEB DEV','APP DEV','CYBER SECURITY']

# # Generate additional names and corresponding skills for skills1
# additional_names = ['John', 'Mary', 'David', 'Sarah', 'Michael']
# additional_skills1 = ['Java', 'HTML', 'CSS', 'Ruby', 'PHP']

# # Extend names and skills1
# names.extend(additional_names)
# skills1.extend(additional_skills1)

# Sample datasets
names = ['John', 'Mary']
skills1 = ['Python', 'Java', 'C++']
skills2 = ['JavaScript', 'React', 'Node.js']

# Extend datasets
additional_names = ['Sarah', 'David']
additional_skills1 = ['Python', 'Java', 'Ruby']
additional_skills2 = ['JavaScript', 'Vue', 'Swift']

names.extend(additional_names)
skills1.extend(additional_skills1)
skills2.extend(additional_skills2)

# Ensure that names, skills1, and skills2 are of equal length by taking the minimum length
min_length = min(len(names), len(skills1), len(skills2))

# Create DataFrames with equal-length data
data1 = {
    'Name': names[:min_length],
    'Skills1': skills1[:min_length]
}

data2 = {
    'Skills2': skills2[:min_length]
}

df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)

# Create a dictionary to map skill2 to the corresponding name
name_dict = {skill2: name for name, skill2 in zip(df1['Name'], df2['Skills2'])}

# Encode skills using LabelEncoder
encoder = LabelEncoder()
encoder.fit(df1['Skills1'].values.tolist() + df2['Skills2'].values.tolist())
num_skills = len(encoder.classes_)

# # Create pairs of skills and labels for training
# skill_pairs = []
# labels = []

# for i, skill2 in enumerate(df2['Skills2']):
#     for j, skill1 in enumerate(df1['Skills1']):
#         skill_pairs.append((skill1, skill2))
#         labels.append(int(skill1 == skill2))  # 1 if similar, 0 if different

# Generate skill pairs
skill_pairs = []
labels = []

for name, skill1_set, skill2_set in zip(names, skills1, skills2):
    for skill1 in skill1_set:
        for skill2 in skill2_set:
             skill_pairs.append((skill1, skill2))
             labels.append(int(skill1 in skill2_set))

# Convert skills to their integer labels
skill_pairs_encoded = [(encoder.transform([s1])[0], encoder.transform([s2])[0]) for s1, s2 in skill_pairs]

# Split the data into training and validation sets
split_ratio = 0.8
split_index = int(len(skill_pairs_encoded) * split_ratio)

train_pairs = skill_pairs_encoded[:split_index]
train_labels = labels[:split_index]
val_pairs = skill_pairs_encoded[split_index:]
val_labels = labels[split_index:]

# Siamese network model
input_shape = (1,)
embedding_dim = 50

input1 = Input(input_shape)
input2 = Input(input_shape)

embedding_layer = Embedding(input_dim=num_skills, output_dim=embedding_dim)

encoded1 = embedding_layer(input1)
encoded2 = embedding_layer(input2)

distance_layer = Lambda(lambda x: tf.abs(x[0] - x[1]))
distance = distance_layer([encoded1, encoded2])

flatten_layer = Flatten()(distance)
dense1 = Dense(32, activation='relu')(flatten_layer)
dense2 = Dense(16, activation='relu')(dense1)
output_layer = Dense(1, activation='sigmoid')(dense2)

model = Model(inputs=[input1, input2], outputs=output_layer)

# Compile the model
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Train the Siamese network
batch_size = 16
epochs = 50

train_data = ([np.array(train_pairs)[:, 0], np.array(train_pairs)[:, 1]], np.array(train_labels))
val_data = ([np.array(val_pairs)[:, 0], np.array(val_pairs)[:, 1]], np.array(val_labels))

model.fit(train_data[0], train_data[1], batch_size=batch_size, epochs=epochs, validation_data=val_data)

# Find closest skill matches
top_matches = []

for i, skill2 in enumerate(df2['Skills2']):
    skill2_encoded = encoder.transform([skill2])[0]

    closest_idx = np.argmin(model.predict([np.array([skill2_encoded] * len(encoder.classes_)), np.array(list(range(num_skills)))])[:, 0])
    closest_skill = encoder.classes_[closest_idx]
    similarity_score = 1.0 - model.predict([np.array([skill2_encoded] * len(encoder.classes_)), np.array(list(range(num_skills)))])[closest_idx, 0]

    # Get the name associated with skill2
    person_name = name_dict.get(skill2, 'Unknown')

    top_matches.append((person_name, skill2, closest_skill, similarity_score))

# Create a Pandas DataFrame for the top matches
top_matches_df = pd.DataFrame(top_matches, columns=['Person', 'Skill2', 'Closest Skill1', 'Similarity Score'])

# Group by 'Skill2' and 'Closest Skill1', then aggregate the 'Person' column to collect names
grouped_matches = top_matches_df.groupby(['Skill2', 'Closest Skill1'])['Person'].apply(list).reset_index()

# Display the DataFrame with top matches
print(grouped_matches)

import numpy as np
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Lambda, Dense, Flatten

# Sample data
names = ['John', 'Mary', 'Sarah', 'David']
skills1 = ['Python', 'Java', 'C++', 'Ruby']
skills2 = ['JavaScript', 'React', 'Vue', 'Swift']

# Generate skill pairs
skill_pairs = []
for n, s1, s2 in zip(names, skills1, skills2):
  for skill in s1:
    for related_skill in s2:
      skill_pairs.append((skill, related_skill))

# Create labels
labels = []
for s1, s2 in skill_pairs:
  labels.append(1 if s1 in skills2 else 0)
labels = np.array(labels)

# Encode skills
encoder = LabelEncoder()
encoder.fit(skills1 + skills2)
encoded_pairs = [(encoder.transform([s1])[0], encoder.transform([s2])[0]) for s1, s2 in skill_pairs]

# Split data
split_idx = int(0.8*len(encoded_pairs))
train_pairs, train_labels = encoded_pairs[:split_idx], labels[:split_idx]
val_pairs, val_labels = encoded_pairs[split_idx:], labels[split_idx:]

# Model
input1 = Input((1,))
input2 = Input((1,))

emb = Embedding(len(encoder.classes_), 50)
enc1 = emb(input1)
enc2 = emb(input2)

distance = Lambda(lambda x: abs(x[0] - x[1]))([enc1, enc2])

x = Dense(32, activation='relu')(distance)
x = Dense(16, activation='relu')(x)
output = Dense(1, activation='sigmoid')(x)

model = Model([input1, input2], output)

# Train
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit([train_pairs[:,0], train_pairs[:,1]], train_labels,
          validation_data=([val_pairs[:,0], val_pairs[:,1]], val_labels))

# Inference...

df1.head(10)

df2

# Dataset 1
# !pip install sentence-transformers

import numpy as np
from sentence_transformers import SentenceTransformer, util

# Dataset 1
people_skills = [{'name': 'John', 'skills': 'Python, Java, C++'},
                 {'name': 'Mary', 'skills': 'JavaScript, React,C'},
                 {'name': 'Mark', 'skills': 'Python, Flask,R'}]

# Dataset 2
target_skills = ['JavaScript', 'Python','C++','Big Data']

# Load the sentence embeddings model (DistilBERT-based)
embedding_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Generate embeddings for dataset 1
embeddings = embedding_model.encode([p['skills'] for p in people_skills])

# Find top matches for each target skill
top_matches = []
for skill in target_skills:
    skill_embedding = embedding_model.encode(skill)

    # Calculate cosine similarities
    scores = util.pytorch_cos_sim(embeddings, skill_embedding)

    # Find the index of the highest similarity
    top_idx = np.argmax(scores)

    name = people_skills[top_idx]['name']
    score = scores[top_idx][0]

    top_matches.append((skill, name, score))

print(top_matches)

import numpy as np
from sentence_transformers import SentenceTransformer, util

# Dataset 1
people_skills = [{'name': 'John', 'skills': 'Python, Java, C++'},
                 {'name': 'Mary', 'skills': 'JavaScript, React, C'},
                 {'name': 'Mark', 'skills': 'Python, Flask, R'},
                 {'name': 'Sarah', 'skills': 'Java, SQL, PHP'},
                 {'name': 'Michael', 'skills': 'C++, Python, Linux'},
                 {'name': 'Alice', 'skills': 'JavaScript, HTML, CSS'}]

# Dataset 2
target_skills = ['JavaScript', 'Python', 'C++', 'Big Data', 'SQL']

# Load the sentence embeddings model (DistilBERT-based)
embedding_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Generate embeddings for dataset 1
embeddings = embedding_model.encode([p['skills'] for p in people_skills])

# Find top matches for each target skill
top_matches = []

for skill in target_skills:
    skill_embedding = embedding_model.encode(skill)

    # Calculate cosine similarities
    scores = util.pytorch_cos_sim(embeddings, skill_embedding)

    # Find all indices with similarity above a threshold (e.g., 0.7)
    threshold = 0.68
    matching_indices = np.where(scores > threshold)[0]

    # Get the names of people with matching skills
    matching_names = [people_skills[idx]['name'] for idx in matching_indices]

    top_matches.append((skill, matching_names))

print(top_matches)

import numpy as np
from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.metrics.pairwise import cosine_similarity

# Dataset 1
people_skills = [{'name': 'John', 'skills': 'Python, Java, C++'},
                 {'name': 'Mary', 'skills': 'JavaScript, React'},
                 {'name': 'Mark', 'skills': 'Python, Flask'}]

# Dataset 2
target_skills = ['JavaScript', 'Python']

# Load DistilBERT tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-nli-stsb-mean-tokens")
model = DistilBertModel.from_pretrained("distilbert-base-nli-stsb-mean-tokens")

# Encode and calculate embeddings for dataset 1
embeddings = []
for person in people_skills:
    text = person['skills']
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy())

# Convert embeddings to numpy array
embeddings = np.array(embeddings)

# Find top matches for each target skill
top_matches = []
for skill in target_skills:
    inputs = tokenizer(skill, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        skill_embedding = model(**inputs).last_hidden_state.mean(dim=1).numpy()

    scores = cosine_similarity(embeddings, skill_embedding.reshape(1, -1))
    top_idx = np.argmax(scores)

    name = people_skills[top_idx]['name']
    score = scores[top_idx][0]

    top_matches.append((skill, name, score))

print(top_matches)

!pip install pandas
!pip install numpy
!pip install sentence_transformers

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util

# Sample Dataset 1
people_skills = [{'name': 'John', 'skills': 'Python, Java, C++'},
                 {'name': 'Mary', 'skills': 'JavaScript, React, C'},
                 {'name': 'Mark', 'skills': 'Python, Flask, R'},
                 {'name': 'Sarah', 'skills': 'Java, SQL, PHP'},
                 {'name': 'Michael', 'skills': 'C++, Python, Linux'},
                 {'name': 'Alice', 'skills': 'JavaScript, HTML, CSS'}]

df1 = pd.DataFrame(people_skills)

# Sample Dataset 2
target_skills = ['JavaScript', 'Python', 'C++', 'Big Data', 'SQL']

# Convert to Pandas DataFrames
# df1 = pd.read_csv('/content/dataset_1_skills.csv')
df2 = pd.DataFrame({'target_skills': target_skills})

# Load the sentence embeddings model (DistilBERT-based)
embedding_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Generate embeddings for dataset 1
embeddings = embedding_model.encode(df1['skills'].tolist(), convert_to_tensor=True)

# Create a dictionary to store matching names for each target skill
top_matches = {skill: [] for skill in target_skills}

for idx, skill in enumerate(target_skills):
    skill_embedding = embedding_model.encode(skill, convert_to_tensor=True)

    # Calculate cosine similarities
    scores = util.pytorch_cos_sim(embeddings, skill_embedding)

    # Find all indices with similarity above a threshold (e.g., 0.7)
    threshold = 0.7
    matching_indices = np.where(scores > threshold)[0]

    # Get the names of people with matching skills
    matching_names = df1['name'].iloc[matching_indices].tolist()

    top_matches[skill] = matching_names

# Display the DataFrames
print("DataFrame 1:")
print(df1)
print("\nDataFrame 2:")
print(df2)

# Display the top matches
print("\nTop Matches:")
for skill, names in top_matches.items():
    print(f"Skill: {skill}, Matching Names: {', '.join(names)}")
    print(len(names))

df1.to_csv('people_skills.csv', index=False)

import requests
import csv

url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSC0dOhvlVgYO9XcPv7qg68xGzSPphj7PrPiM0z9M3HxF5JS677dbgV44clx7NuVRG_SepAzMm4feBC/pub?output=csv"
response = requests.get(url)
if response.status_code == 200:
    csv_content = response.text
    with open("student_data.csv", "w", newline='') as csv_file:
        csv_file.write(csv_content)

url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRHpYw-aN4tkA9lrG2hWe_i9PQLo1T3A8_ODqn-_co2AUrwUXSD0KJ5IqLcFyxtUdiKBk8qwbBCPOaC/pub?gid=1944232338&single=true&output=csv"
response = requests.get(url)
if response.status_code == 200:
    csv_content = response.text
    with open("prof_data.csv", "w", newline='') as csv_file:
        csv_file.write(csv_content)

df1 = pd.read_csv('/content/student_data.csv')
df2 = pd.read_csv('/content/prof_data.csv')
df_meta = df2['SKILLS'].str.split(',')
first_row_list = df_meta.at[0]

df1["SKILLS"]

first_row_list

data = []
embeddings = embedding_model.encode(df1['SKILLS'].tolist(), convert_to_tensor=True)
top_matches = {skill: [] for skill in first_row_list}

for idx, skill in enumerate(first_row_list):
    skill_embedding = embedding_model.encode(skill, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(embeddings, skill_embedding)
    threshold = 0.7
    matching_indices = np.where(scores > threshold)[0]
    matching_names = df1['Full Name'].iloc[matching_indices].tolist()
    top_matches[skill] = matching_names

for skill, names in top_matches.items():
    data.extend([(skill, name) for name in names])

data













import numpy as np
from sentence_transformers import SentenceTransformer, util

# Dataset 1
people_skills = [{'name': 'John', 'skills': 'Python, Java, C++'},
                 {'name': 'Mary', 'skills': 'JavaScript, React, C'},
                 {'name': 'Mark', 'skills': 'Python, Flask, R'},
                 {'name': 'Sarah', 'skills': 'Java, SQL, PHP'},
                 {'name': 'Michael', 'skills': 'C++, Python, Linux'},
                 {'name': 'Alice', 'skills': 'JavaScript, HTML, CSS'}]

# Dataset 2
target_skills = ['JavaScript', 'Python', 'C++', 'Big Data', 'SQL']

# Load the sentence embeddings model (DistilBERT-based)
embedding_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Generate embeddings for dataset 1
embeddings = embedding_model.encode([p['skills'] for p in people_skills])

# Find top matches for each target skill
top_matches = []

for skill in target_skills:
    skill_embedding = embedding_model.encode(skill)

    # Calculate cosine similarities
    scores = util.pytorch_cos_sim(embeddings, skill_embedding)

    # Find all indices with similarity above a threshold (e.g., 0.7)
    threshold = 0.68
    matching_indices = np.where(scores > threshold)[0]

    # Get the names of people with matching skills
    matching_names = [people_skills[idx]['name'] for idx in matching_indices]

    top_matches.append((skill, matching_names))

print(top_matches)

